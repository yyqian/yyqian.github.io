---
title: 统计学习方法 - 感知机
date: 2014-08-27 11:13:00
permalink: 1409109180000
tags: 机器学习
---

这里主要是对李航的《统计学习方法》中的内容做一些浅显的解释，因为这本书内容比较精干，需要一定的知识背景，理解起来不是很容易。

先举个简单的例子介绍下感知机(perceptron)算法所能解决的问题：

上帝要跟我们玩个游戏：它手上有很多黑白两色的棋子，然后它不断地把这些棋子扔到一个棋盘上，扔的时候它总是倾向于把黑棋子和白棋子分别扔到棋盘上两个不同的区域。游戏开始时，上帝先扔了100个棋子，我们可以看到棋盘上形成了黑白两片，然后上帝把棋盘清空，继续扔棋子，但这个时候它把我们的眼睛蒙起来，只告诉我们棋子所在位置，要让我们通过棋子的位置来判断棋子的颜色。

玩这个游戏的一个简单方法是：在上帝扔完100个棋子之后，我们可以根据黑白两垛棋子的位置，用一根木棍把棋盘分隔成两个区域，接下来上帝继续扔的时候，我们只需要知道棋子出现在木棍分隔出来的哪个区域，就能推断这个棋子的颜色了。

如果让计算机来解决这个问题，难点就在于如何根据这两垛棋子的位置，来判断这根木棍放置的位置。感知机算法核心的问题就是如何计算出这根木棍摆放的位置。

上面这些描述如果用简单的数学来概括的话，就是在一个平面上，根据若干已经分类好的数据点的分布情况，计算出一条直线，来分隔出平面的两个区域。根据这两个区域，我们可以用新数据点的位置来推断该数据点的类别。

接下来，我们来把这个问题从平面推广到空间（也就是从二维到三维）。想象一下，现在这些棋子是可以漂浮在空间里的，但上帝还是倾向于把黑白棋子扔到两个不同的区域，这时候我们可以用一个平面来分隔这两个区域。概括来说，二维平面的数据点可以用一维的直线来分隔，三维空间的数据点可以用二维平面来分隔。对于其他类似的分类问题，我们可能会遇到n维空间里的数据点，这个时候我们可以用一个n-1维的所谓的“超平面”来将这些数据点分隔成两个区域（n-1维的超平面实际就是对n维空间加一个线性的约束条件）。

至于如何将n维空间的数据点用一个n-1维的超平面来分隔，就请移步李航的《统计学习方法》第2章吧。
<!-- more -->
接下来说一下这个算法的局限性和值得思考的问题：首先，这个算法解决的是二分类问题（棋子只有黑白两种），如果我们的棋子有更多的颜色分类，分别分布在多个区域，如何划分这些区域？另外，如果有若干黑棋误扔在白棋堆里或者反过来，那么就无法将两类棋子完全分隔开来，这时候怎么办？最后，前面提到了“线性的约束条件”，这个算法的进行的分类我们称为线性分类，我们可以观察到我们采用的分割线是直线，分割面是平面，但如果采用曲线或曲面来分割，效果是否更好？是否可以实现？

另外，在这个游戏过程中，上帝让我们先观察它扔100个棋子的过程，我们可以理解为一个学习的过程，棋子的位置我们可以看作是输入，棋子的颜色看作是输出，通过这个学习我们要建立起一个将输入转变为输出的模型，这100组输入输出数据我们称之为训练数据集。在蒙住眼睛后，我们可以通过将上帝告诉我们的位置信息作为输入，通过这个模型得到输出，即棋子的颜色。这个过程对于人来说是一个“触类旁通”的过程，但如果把上述的“我们”换成“机器”，这就是一个机器学习的过程。

最后贡献一段自己写的python代码，程序会随机生成若干红点和蓝点，然后根据这些数据点，程序会自动找出一条划分红蓝两个区域的直线。注意，随机生成的数据点不都是可以分隔成两个区域的，可以多运行几次测试一下。这里的输入是2维空间的，但核心部分是适用于n维空间的。初学python，见笑：

	import numpy as np
	import matplotlib.pyplot as plt
	import random as rd
	import time
	###################################
	def right_class(x_i, y_i, w, b):
    	temp = 0
    	N = len(x_i)
    	for j in range(N):
        	temp = temp + x_i[j] * w[j]
    		return y_i*(temp + b)

	def update_parameters(x, y, w, b, eta):
    	N = len(y)
    	dim = len(w)
    	list_rd = range(N)
    	rd.shuffle(list_rd)
    	for k in range(N):
        	i = list_rd[k]
        	if (right_class(x[i], y[i], w, b) <= 0): #wrong class
            	for j in range(dim):
                	w[j] = w[j] + eta * y[i] * x[i][j]
            	b = b + eta * y[i]
            	break
        	else:
            	pass
    	return b

	def loss(x, y, w, b):
    	N = len(y)
    	dim = len(w)
    	L = 0
    	wrong = 0
    	for i in range(N):
        	L_i = right_class(x[i], y[i], w, b)
        	if (L_i <= 0): #wrong class
            	wrong = wrong + 1
            	L = L - L_i
        	else:
            	pass
    	if (wrong == 0):
        	L = -1
    	return L

	def generate_xy():
    	x = []
    	y = []
    	for i in range(5):
        	x.append([rd.random()*2,rd.random()*2])
        	y.append(1)
    	for i in range(5):
        	x.append([rd.random()*6,rd.random()*6])
        	y.append(-1)
    	return x, y

	def plot_result(x, y, w, b, L):
    	ax1 = plt.subplot(211)
    	ax2 = plt.subplot(212)
    	line_x = np.arange(min([x2[0] for x2 in x]), max([x2[0] for x2 in x]), 0.001)
    	line_y = - (w[0] * line_x + b)/w[1]
    	plt.sca(ax1)
    	plt.plot(line_x, line_y, 'g')
    	plt.xlabel('$x^{(1)}$')
    	plt.ylabel('$x^{(2)}$')
    	for j in range(len(y)):
        	if (y[j] == 1):
            	plt.plot(x[j][0],x[j][1],'or')
        	elif (y[j] == -1):
            	plt.plot(x[j][0],x[j][1],'ob')
        	else:
            	print('y[' + str(j) + '] value is invalid!')
    	plt.sca(ax2)
    	plt.plot(L)
    	plt.xlabel('iteration')
    	plt.ylabel('loss')
    	plt.show()
	#########################
	start = time.clock()
	[x, y] = generate_xy()
	w = [0, 0]
	b = 0
	eta = 0.1
	iteration_max = 100;
	L=[]
	for i in range(iteration_max):
    	L.append(loss(x, y, w, b))
    	if (L[i] == -1)|(i==(iteration_max - 1)):
        	elapsed = (time.clock() - start)
        	print("Time used:" + str(elapsed) + 's')
        	if (L[i] == -1):
            	L[i] = 0
        	plot_result(x, y, w, b, L)
        	break
    	else:
        	b = update_parameters(x, y, w, b, eta)
