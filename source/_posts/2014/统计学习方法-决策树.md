---
title: 统计学习方法 - 决策树
date: 2014-09-05 16:34:00
permalink: 1409906040000
tags: 机器学习
---

决策树模型一个比较容易理解的例子就是信用卡申请：当我们申请信用卡时，会被问一堆问题，其中包括是否有房子、有工作，月收入，教育程度，是否结婚等等，你的这些每项个人信息都会影响你申请的结果（通过或拒绝）。然而，这些各项信息对于结果的影响权重是不同的，譬如说现在有房子比有工作更能说明你财务状况不错。因此，在评估的过程中应当首先考虑权重大的因素，如果几个权重大的因素都满足要求，我们可以忽略剩余的其他因素，提前结束评估让申请通过，反之亦然，这样在一定程度上能加快评估过程。

基于这个想法，我们可以考察一个测试数据集，通过计算这个数据集各个因素的权重，构造一个树形结构，每个节点都是一个因素（譬如：工作，教育程度），节点的分支代表这个因素的取值（譬如：有/没有，本科/硕士/博士），节点分支连接的子节点是满足该分支条件的样本。这样我们把权重最大的因素放在根节点，然后按权重从大到小生出各个节点，当该节点所包含的样本都是同一类的时，这个节点就是叶节点，可以唯一地确定结果。（这个决策树的生成过程建议参考书本，我想不出通俗的语句来解释。）

这里比较重要的一点是“权重”到底是什么？如何来定量地计算它？书本从信息论和概率统计的角度引入了“熵(entropy)”，用于计算这个"权重"。熵是衡量系统混乱度/不确定性的量，它跟信息量是有一定关联的（成正比关系还是反比关系个人认为是取决于如何定义信息量）。我们所要做的是先计算出整个数据集的熵，再计算出给定某个因素之后的数据集的熵，前者和后者的差被称为“信息增益”。这是因为给定该因素后，系统的熵减小了，也就是说不确定性小了，更容易分类了（相当于获得了更多信息量），因此熵的差值等价于信息量的增益。所以以上在构造决策树的过程中，我们需要不断地计算各种因素条件下的“信息增益”来作为“权重”，并进行对比选择。

最后，当完整的决策树构造好后，我们还有可能需要进行“剪枝”。这是因为完整的决策树往往过于复杂，对于测试数据的分类过程中可能导致过拟合现象，因此我们需要降低模型复杂度，剪去一些分支，在对训练数据和测试数据的拟合程度上找到一个平衡点。

CART算法：这个算法个人认为是决策树的简化版，这里的“权重”用Gini指数来代替，并且生成的决策树是一棵二叉树，即一个节点只有两个分支。对于有多个分支的，譬如说年龄（可分为青年，中年，老年），我们可以分别计算以下三种分类的Gini指数：青年和非青年，中年和非中年，老年和非老年。通过比较这三种分类的Gini指数，选指数最小的分类方法来生成两个分支。

程序方面，感觉结合数据库会更方便，我自己没有写相应的代码。