---
title: 统计学习方法 – 朴素贝叶斯法
date: 2014-08-29 11:23:00
permalink: 1409282580000
tags: 机器学习
---

朴素贝叶斯法(naive Bayes)是基于贝叶斯定理的，区别在于多了一个“条件独立性假设”，目的是减少系统参数，继而降低计算难度。所以这里贝叶斯理论相当于是一个模型，朴素贝叶斯法是一个实现的策略，具体算法上貌似没什么tricky的东西。要理解这个方法要先理解贝叶斯理论，这部分理论PRML第一章讲得应该是比较清楚的。

这里给出一个书中例4.1的Python代码，使用的训练数集和书中是相同的，可以测试任何输入。另外，程序是为任意维度、长度、分类数设计的，应该是有一定可扩展性的。

	'''
	Naive Bayes Algorithm
	'''
	def loss_I1(x, a):
    	if (x == a):
        	return 1.0
    	else:
        	return 0.0
	def loss_I2(x, a, y, c):
    	if ((x == a) and (y == c)):
        	return 1.0
    	else:
        	return 0.0
	def product(P):
    	output = 1.0
    	for P_i in P:
        	output *= P_i
    	return output
	def x2i(x, A):
    	dim = len(x)
    	index = [[i for i, val in enumerate(A[0][k]) if (val == x[k])][0] for k in range(dim)]
    	'''
    	index = []
    	for k in range(dim):      
        	for i, val in enumerate(A[0][k]):
            	if (val == x[k]):
                	index.append(i)
                	break
            	else:
                	pass
    	'''
    	return index
	def max_index(x):
    	max_val = -999
    	for i, val in enumerate(x):
        	if (val > max_val):
            	max_val = val
            	index = i
    	return index
	##################
	S = 'S'
	M = 'M'
	L = 'L'
	T = [[[1, S],-1], [[1, M],-1], [[1, M],1], [[1, S],1], [[1, S],-1], [[2, S],-1], [[2, M],-1], [[2, M],1], [[2, L],1], [[2, L],1], [[3, L],1], [[3, M],1], [[3, M],1], [[3, L],1], [[3, L],-1]]
	N = len(T)
	dim = len(T[0][0])
 
	A = [[[1,2,3],[S,M,L]],[1,-1]]
	len_class = len(A[1])
	len_val = []
	for x in A[0]:
    	len_val.append(len(x))
 
	print('X1 belongs to {1, 2, 3}, X2 belongs to {S, M, L}.')
	input_x = []
	input_x.append(int(raw_input('X1 = ')))
	input_x.append(raw_input('X2 = ').upper())
	input_i = x2i(input_x, A)
	####################
	P_Y = []
	for i in range(len_class):
    	P_Y.append(sum([loss_I1(t[1], A[1][i]) for t in T])/N)
 
	P_XY = []
	for j in range(dim):
    	P_XY.append([])
    	for l in range(len_val[j]):
        	P_XY[j].append([])
        	for k in range(len_class):
            	P_XY[j][l].append(sum([loss_I2(T[i][0][j], A[0][j][l], T[i][1], A[1][k]) for i in range(N)])/(N*P_Y[k]))
 
	P_output = []
	for k in range(len_class):
    	P_output.append( P_Y[k]*product([P_XY[j][input_i[j]][k] for j in range(dim)]) )
 
	output_y = A[1][max_index(P_output)]
	for k in range(len_class):
    	print ('P(Y = %d) = %f'%(A[1][k],P_output[k]))
	print ('The class of input is: ' + str(output_y) + '.')
	raw_input('Press Enter to exit...')
